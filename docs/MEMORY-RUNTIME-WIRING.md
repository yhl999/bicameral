# Memory Runtime Wiring

This document defines the runtime backend switch contract for memory retrieval.

## Runtime model

Two backend profiles are supported:

- `qmd_primary` (production default)
- `graphiti_primary` (operator opt-in)

Profiles are declared in:

- `config/runtime_memory_backend_profiles.json`

Current active state is stored in:

- `config/.runtime_memory_backend_state.json`

## Operator commands

Run from repository root.

```bash
python3 scripts/runtime_memory_backend_status.py
python3 scripts/runtime_memory_backend_switch.py --target graphiti_primary --dry-run
python3 scripts/runtime_memory_backend_switch.py --target qmd_primary --dry-run
python3 scripts/runtime_memory_backend_switch.py --target graphiti_primary --execute
python3 scripts/runtime_memory_backend_switch.py --revert --execute
```

## Guardrails

- group-safe gating must stay enabled in all active profiles
- shadow compare should remain enabled during cutover
- one-command revert must always be available after a switch

## Public/private split

Public repo contains generic switch/status framework and example/default profile config.

Private operational overlays may replace profile values at deploy time (for environment-specific behavior), but should not change the switch contract surface.

## Security considerations

The ingest pipeline processes raw session transcripts, memory files, and conversation
data that inherently contain personally identifiable information (PII).  This is by
design — the pipeline's purpose is to extract knowledge from these sources.

**Directories that contain PII at runtime** (all gitignored):

- `evidence/` — parsed evidence documents
- `state/` — ingest registry DB, queue state
- `logs/` — worker execution logs

**Input validation:** All user-supplied identifiers (`group_id`, `session_key`,
`source`) are validated against a strict allowlist pattern
(`[A-Za-z0-9][A-Za-z0-9._:@-]{0,254}`) before use in subprocess arguments or
database keys.  See `ingest/queue.py:validate_identifier()`.

**Error handling:** Worker error messages use structured tags (`error_type:ClassName`)
rather than raw exception messages to avoid leaking internal state.

**Subprocess execution:** All subprocess calls use list-form arguments (never
`shell=True`), preventing shell injection even if an identifier were to bypass
validation.

## OM Fast-Write Wiring

The OM fast-write path is a separate write lane that bypasses the MCP server's
ingestion queue. Use it when you need sub-second writes from live transcript streams.

### Enabling Fast-Write

```bash
# 1. Wire the state file into your runtime repo
python3 scripts/om_fast_write.py set-state \
  --runtime-repo /path/to/runtime-repo \
  --enabled \
  --reason "hook_wired"

# 2. Write messages directly
python3 scripts/om_fast_write.py write \
  --session-id "<session_id>" \
  --role user \
  --content "<message text>" \
  --created-at "2026-02-26T12:00:00Z"
```

### Fast-Write Integration Points

| Hook | How |
|---|---|
| **OpenClaw plugin** | `om_fast_write.py write --payload-file <tmpfile>` on each transcript message |
| **Cron drain** | `om_fast_write.py write --payload-json '{"source_session_id":...}'` |
| **Runtime repo state** | `state/om_fast_write_state.json` (gitignored) — read by runtime health checks |

### OM Wiring Paths

```
Transcript message
        │
        ├─► [MCP path]   mcp_ingest_sessions.py → Graphiti extraction queue
        │                (sets graphiti_extracted_at on Message)
        │
        └─► [OM path]    om_fast_write.py write  → Neo4j Message/Episode nodes
                                │
                                ▼
                         om_compressor.py        → OMNode extraction
                                │
                                ▼
                         om_convergence.py       → Lifecycle state machine
                                │
                                ▼
                         promotion_policy_v3.py  → CoreMemory (on corroboration)
```

Both paths are independent and can run concurrently. A `Message` node can be
processed by both Graphiti (sets `graphiti_extracted_at`) and OM (sets `om_extracted`).

GC eligibility requires **both** `graphiti_extracted_at IS NOT NULL` and `om_extracted = true`,
so messages on only one path are retained until the other path also completes.

### Environment Variables

| Var | Default | Description |
|---|---|---|
| `OM_EMBEDDING_MODEL` | `embeddinggemma` | Embedding model |
| `OM_EMBEDDING_DIM` | `768` | Expected vector dimension |
| `EMBEDDER_BASE_URL` | `http://localhost:11434/v1` | OpenAI-compatible embedding endpoint |
| `RUNTIME_REPO_ROOT` | (none) | If set, fast-write updates the state file automatically |

---

## Runtime pack injection policy (Graphiti/CLR)

Runtime pack routing is performed by `scripts/runtime_pack_router.py`.

Canonical policy controls now enforced in code:
- Multi-group retrieval matrix per pack via `retrieval.group_ids_by_mode`.
- ChatGPT lane gating via per-profile `chatgpt_mode = off|scoped|global`.
- Scoped is the intended safe default when mode is omitted.
- Engineering learnings can be materialized at runtime (`--materialize`) from latest loop artifacts.

See also: `docs/runbooks/runtime-pack-overlay.md`.

## Canonical runtime checkout

Operational runtime should execute from the canonical runtime checkout linked by:

- `tools/graphiti -> ../projects/graphiti-openclaw-runtime`

Apply private overlay before operations:

```bash
/Users/archibald/clawd/projects/graphiti-openclaw-private/scripts/apply-overlay.sh \
  /Users/archibald/clawd/projects/graphiti-openclaw-runtime
```
